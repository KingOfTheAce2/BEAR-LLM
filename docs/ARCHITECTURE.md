# ğŸ—ï¸ BEAR AI LLM - System Architecture

**Understanding the Infrastructure: LLM vs RAG vs PII Detection**

---

## ğŸ“Š High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       BEAR AI APPLICATION                        â”‚
â”‚                     (Tauri Desktop - Rust + React)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚               â”‚               â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ LLM Manager  â”‚ â”‚ RAG Engine â”‚ â”‚ PII Detector   â”‚
        â”‚ (Chat/Gen)   â”‚ â”‚ (Search)   â”‚ â”‚ (Privacy)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Component 1: LLM Manager (Text Generation)

**File:** `src-tauri/src/llm_manager_production.rs`

### Purpose
Handles **text generation, chat, and question answering**. This is the "brain" that thinks and talks.

### Models Used
- **TinyLlama-1.1B** (small, fast, 2GB)
- **Mistral-7B** (balanced, 5GB)
- **Llama2-13B** (high quality, 10GB+)

### Technology Stack
- **Candle Framework** (HuggingFace/Rust)
- **GGUF Format** (quantized models)
- **GPU/CPU Inference** (CUDA or fallback)

### Capabilities
âœ… Generate human-like text
âœ… Chat and converse
âœ… Answer questions
âœ… Reasoning and logic
âœ… Code generation

âŒ Cannot search documents efficiently
âŒ Cannot convert text to vectors

### Example Usage
```rust
User: "What is a tort in legal terms?"
LLM: "A tort is a civil wrong that causes harm or loss to another
      person, for which the law provides a remedy..."
```

### Model Selection
Users can download and switch between LLM models from HuggingFace:
- Browse models in the UI
- Download from HuggingFace Hub
- Switch active model at runtime
- Models stored in: `%LOCALAPPDATA%\bear-ai-llm\models\`

---

## ğŸ“š Component 2: RAG Engine (Document Search)

**File:** `src-tauri/src/rag_engine_production.rs`

### Purpose
Handles **document retrieval and semantic search**. This is the "library" that finds relevant information.

### Models Used (Embeddings - NOT LLMs!)
- **BGE-Small-EN-V1.5** (default, 150MB, 384-dim)
- **Legal-BERT** (optional, 440MB, 768-dim, legal-specific)
- **Sentence-T5** (optional, 670MB, best quality)

### Technology Stack
- **FastEmbed Library** (efficient embeddings)
- **Vector Database** (in-memory + SQLite)
- **Cosine Similarity** (semantic matching)

### Capabilities
âœ… Convert text to vector embeddings
âœ… Semantic search (find similar content)
âœ… Document chunking and indexing
âœ… Hybrid search (vector + keyword)

âŒ Cannot generate text
âŒ Cannot chat or answer questions
âŒ Cannot reason or understand context

### Example Usage
```rust
// Input: Legal contract text
"The licensee shall indemnify the licensor..."

// Output: Vector embedding (384 numbers)
[0.234, -0.456, 0.678, 0.123, ..., 0.891]

// Used for: Finding similar clauses in other documents
```

### How RAG Works
1. **Indexing:** Document â†’ Split into chunks â†’ Convert to embeddings â†’ Store
2. **Search:** User query â†’ Convert to embedding â†’ Find similar chunks â†’ Return top matches
3. **Generation:** LLM reads retrieved chunks â†’ Generates contextual answer

---

## ğŸ”’ Component 3: PII Detector (Privacy Protection)

**File:** `src-tauri/src/pii_detector_production.rs`

### Purpose
Detects and scrubs personally identifiable information (PII) from documents.

### Technology
- **Built-in Regex** (always available, basic)
- **Microsoft Presidio** (optional, enterprise-grade, Python)
- **OpenPipe PII-Redact** (optional, transformer models)

### Detected Entities
- Names (person, organization)
- Social Security Numbers
- Credit card numbers
- Email addresses
- Phone numbers
- Physical addresses
- Dates of birth
- Medical record numbers

---

## ğŸ”„ How Components Work Together

### Example: User Asks About Uploaded Contract

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User: "What does my contract say about liability limits?"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: PII Detector                                        â”‚
â”‚ â€¢ Scan query for sensitive info (none found)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 2: RAG Engine                                          â”‚
â”‚ â€¢ Convert query to embedding vector                         â”‚
â”‚ â€¢ Search document database for similar chunks               â”‚
â”‚ â€¢ Return top 5 relevant contract sections                   â”‚
â”‚ â†’ "Section 8.3: Limitation of Liability..."                 â”‚
â”‚ â†’ "The Company's total liability shall not exceed $10,000"  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 3: LLM Manager                                         â”‚
â”‚ â€¢ Receive retrieved contract sections                       â”‚
â”‚ â€¢ Generate human-readable answer                            â”‚
â”‚ â†’ "According to Section 8.3 of your contract, liability    â”‚
â”‚    is limited to $10,000 for any claims arising from..."    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Key Differences: LLM vs Embedding Model

| Feature | LLM (Mistral, Llama) | Embedding Model (BGE) |
|---------|---------------------|----------------------|
| **Purpose** | Text generation | Vector search |
| **Size** | 2-13 GB | 150-670 MB |
| **Output** | Human text | Numeric vectors |
| **Can chat?** | âœ… Yes | âŒ No |
| **Can search docs?** | âŒ No (inefficient) | âœ… Yes (purpose-built) |
| **Training** | Language understanding | Semantic similarity |
| **Speed** | Slower (inference) | Fast (vector math) |
| **Use case** | "Answer this question" | "Find similar content" |

---

## ğŸ¯ Why Two Separate Systems?

### âŒ LLM Alone (No RAG)
**Problem:** LLM doesn't know your documents
```
User: "What's in my contract?"
LLM: "I don't have access to your contract, but generally contracts include..."
```

### âŒ RAG Alone (No LLM)
**Problem:** Can find documents but can't explain them
```
User: "What's in my contract?"
RAG: [Returns raw text chunks with no explanation]
```

### âœ… LLM + RAG (Both Working Together)
**Solution:** Smart assistant that knows your documents
```
User: "What's in my contract?"
RAG: [Finds relevant sections]
LLM: "Your contract includes these key terms: 1) Payment of $5,000..."
```

---

## ğŸ”§ Switching Models

### LLM Model Switching (Already Available)
1. Open BEAR AI
2. Click "Model Selection"
3. Browse HuggingFace models
4. Download and activate

**Models stored:** `%LOCALAPPDATA%\bear-ai-llm\models\`

### RAG Model Switching (Coming in v1.0.13)
1. Open Settings â†’ RAG Configuration
2. Select embedding model:
   - **BGE-Small-EN-V1.5** (default, fast, general-purpose)
   - **Legal-BERT** (legal-specific, better for contracts)
   - **Sentence-T5** (best quality, larger size)
3. Download and switch (requires re-indexing documents)

**Models stored:** `.fastembed_cache\`

---

## ğŸ“ File System Layout

```
%LOCALAPPDATA%\bear-ai-llm\
â”œâ”€â”€ models\                    # LLM models (2-13GB each)
â”‚   â”œâ”€â”€ TinyLlama-1.1B\
â”‚   â”œâ”€â”€ Mistral-7B\
â”‚   â””â”€â”€ Llama2-13B\
â”œâ”€â”€ embeddings\                # Embedding models (150-670MB)
â”‚   â””â”€â”€ .fastembed_cache\
â”‚       â””â”€â”€ BAAI__bge-small-en-v1.5\
â”œâ”€â”€ rag_index\                 # Document database
â”‚   â”œâ”€â”€ documents.db           # SQLite metadata
â”‚   â””â”€â”€ embeddings.json        # Vector storage
â”œâ”€â”€ presidio\                  # PII models (optional)
â”‚   â””â”€â”€ en_core_web_sm\
â””â”€â”€ cache\                     # Temporary files
```

---

## ğŸš€ Performance Characteristics

### Startup Time
- **Without RAG preload:** <5 seconds
- **With RAG preload:** 10-30 seconds (model loading)
- **Solution:** Download RAG model during setup wizard

### Memory Usage
| Component | RAM Usage | VRAM (GPU) |
|-----------|-----------|------------|
| Base App | 200-300 MB | - |
| RAG Engine | 500-800 MB | Optional |
| LLM (7B) | 4-6 GB | 4-8 GB |
| Total (typical) | 5-7 GB | 4-8 GB |

### Disk Space
- **Base installation:** ~500 MB
- **RAG embeddings:** 150-670 MB (per model)
- **LLM models:** 2-13 GB (per model)
- **Document index:** Variable (depends on uploads)

---

## ğŸ” Security & Privacy

### 100% Local Processing
- âœ… All LLM inference on-device
- âœ… All RAG searches local
- âœ… No cloud API calls
- âœ… No telemetry or tracking

### Data Storage
- âœ… Documents encrypted at rest (SQLite)
- âœ… PII automatically detected and flagged
- âœ… Optional Presidio for enterprise-grade protection

### Network Access
- âš ï¸ Only for model downloads from HuggingFace
- âš ï¸ Optional auto-updates (user controlled)
- âœ… Can operate fully offline after setup

---

## ğŸ› ï¸ Technology Stack Summary

| Layer | Technology |
|-------|-----------|
| **Frontend** | React + TypeScript + Tailwind CSS |
| **Backend** | Rust (Tauri 2.x) |
| **LLM Runtime** | Candle (HuggingFace) |
| **RAG Engine** | FastEmbed + SQLite |
| **PII Detection** | Regex + Microsoft Presidio (Python) |
| **Vector Math** | Cosine similarity |
| **Database** | SQLite (bundled) |
| **Desktop Framework** | Tauri 2.x |

---

## ğŸ“š Further Reading

- [README.md](../README.md) - User guide and installation
- [CONTRIBUTE.md](../CONTRIBUTE.md) - Contribution guidelines
- [Tauri Documentation](https://tauri.app)
- [Candle Framework](https://github.com/huggingface/candle)
- [FastEmbed](https://github.com/qdrant/fastembed)
- [Microsoft Presidio](https://github.com/microsoft/presidio)

---

**Version:** 1.0.12
**Last Updated:** 2025-09-30
**Author:** Ernst van Gassen
