[package]
name = "bear-ai-llm"
version = "1.0.57"
description = "A secure, private AI assistant for legal professionals"
authors = ["Ernst van Gassen <ernst@example.com>"]
edition = "2021"

[build-dependencies]
tauri-build = { version = "2.4.1", features = [] }

[dependencies]
tauri = { version = "2.4.1", features = ["tray-icon"] }
tauri-plugin-fs = "2.4.2"
tauri-plugin-dialog = "2.4.0"
tauri-plugin-shell = "2.3.1"
tauri-plugin-os = "2.3.1"
tauri-plugin-updater = "2.4.0"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
tokio = { version = "1", features = ["full"] }
reqwest = { version = "0.12", features = ["json"] }
regex = "1"
lazy_static = "1.5"
pulldown-cmark = "0.12"  # Markdown parser for model cards
once_cell = "1.19"
anyhow = "1.0"
tempfile = "3.8"  # Secure atomic temporary file creation
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt", "json"] }
tracing-appender = "0.2"
sentry = { version = "0.32", default-features = false, features = ["backtrace", "contexts", "panic", "reqwest", "rustls"] }
sysinfo = "0.37"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1", features = ["v4", "serde"] }
async-trait = "0.1"
hf-hub = { version = "0.4", features = ["tokio"] }
dirs = "6.0"
urlencoding = "2.1"
walkdir = "2.5"
nvml-wrapper = "0.11"
zip = "0.6"
# AI/ML dependencies - Pure Rust inference (no C++ build issues)
candle-core = "0.8"  # CUDA support optional, enabled at runtime if available
candle-nn = "0.8"
candle-transformers = "0.8"  # GGUF support is built-in via quantized_llama module
tokenizers = "0.21.0"
# Removed llama-cpp-2 - migrated to pure Rust Candle for reliable Windows builds
# Note: GPU acceleration works without CUDA feature - Candle auto-detects at runtime
# Vector database
# faiss = { version = "0.12", optional = true } # Disabled - using fastembed instead
# Better embeddings - Use fastembed with compatible features
fastembed = { version = "5.2.0", default-features = false, features = ["online"] }

[patch.crates-io]
ort = { version = "=2.0.0-rc.10", default-features = false, features = [] }
ort = { version = "=2.0.0-rc.10", default-features = false, features = [] }
# Note: ort (ONNX Runtime) is brought in by gline-rs and fastembed - no direct dependency needed
pdf-extract = "0.7"
rusqlite = { version = "0.31", features = ["bundled-sqlcipher-vendored-openssl"] }
r2d2 = "0.8"  # Connection pooling
r2d2_sqlite = { version = "0.24", default-features = false }  # SQLite connection pool manager (compatible with rusqlite 0.31)
keyring = "3.6"  # Secure OS keychain integration for encryption keys
rand = "0.8"  # Cryptographically secure random number generation
ring = "0.17"  # FIPS-compatible cryptography for chat encryption
argon2 = "0.5"  # Key derivation for per-user encryption
calamine = { version = "0.26", features = ["dates"] }
docx-rs = "0.4"

# Export and compliance dependencies
printpdf = "0.7"  # PDF export
age = "0.10"  # Encryption for data portability
zeroize = "1.7"  # Memory scrubbing for security
sha2 = "0.10"  # Hash functions for integrity checks
hex = "0.4"  # Hex encoding for hashes
# Legacy format support
encoding_rs = "0.8"
cfb = "0.9"  # Compound File Binary format for DOC/PPT
toml = "0.8"  # For PII exclusions config

# Windows-specific dependencies for GPU detection
[target.'cfg(target_os = "windows")'.dependencies]
wmi = "0.13"
windows = { version = "0.58", features = [
    "Win32_System_Com",
    "Win32_System_Ole",
    "Win32_System_Variant",
    "Win32_Foundation",
] }

[features]
default = ["custom-protocol"]
custom-protocol = ["tauri/custom-protocol"]
# CUDA is optional - only enable if building on system with CUDA toolkit
# For most users, CPU fallback is automatic and works fine
cuda = ["candle-core/cuda"]

[lib]
name = "bear_ai_llm"
path = "src/lib.rs"

[[bin]]
name = "bear-ai-llm"
path = "src/main.rs"
