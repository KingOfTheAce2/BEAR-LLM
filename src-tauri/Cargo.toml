[package]
name = "bear-ai-llm"
version = "1.0.57"
description = "A secure, private AI assistant for legal professionals"
authors = ["Ernst van Gassen <ernst@example.com>"]
edition = "2021"

[build-dependencies]
tauri-build = { version = "2.4.1", features = [] }

[dependencies]
tauri = { version = "2.4.1", features = ["tray-icon"] }
tauri-plugin-fs = "2.4.2"
tauri-plugin-dialog = "2.4.0"
tauri-plugin-shell = "2.3.1"
tauri-plugin-os = "2.3.1"
tauri-plugin-updater = "2.4.0"
serde = { version = "1", features = ["derive"] }
serde_json = "1"
tokio = { version = "1", features = ["full"] }
reqwest = { version = "0.12", features = ["json"] }
regex = "1"
lazy_static = "1.5"
pulldown-cmark = "0.12"  # Markdown parser for model cards
once_cell = "1.19"
anyhow = "1.0"
tempfile = "3.8"  # Secure atomic temporary file creation
tracing = "0.1"
tracing-subscriber = { version = "0.3", features = ["env-filter", "fmt", "json"] }
tracing-appender = "0.2"
sentry = { version = "0.32", default-features = false, features = ["backtrace", "contexts", "panic", "reqwest", "rustls"] }
sysinfo = "0.37"
chrono = { version = "0.4", features = ["serde"] }
uuid = { version = "1", features = ["v4", "serde"] }
async-trait = "0.1"
hf-hub = { version = "0.4", features = ["tokio"] }
dirs = "6.0"
urlencoding = "2.1"
walkdir = "2.5"
nvml-wrapper = "0.11"
zip = "0.6"
# AI/ML dependencies - Pure Rust inference (no C++ build issues)
candle-core = "0.8"  # CUDA support optional, enabled at runtime if available
candle-nn = "0.8"
candle-transformers = "0.8"  # GGUF support is built-in via quantized_llama module
tokenizers = "0.21.0"

# Database
rusqlite = { version = \"0.30\", features = [\"bundled\"] }\nr2d2 = \"0.8\"\nr2d2_sqlite = { version = \"0.23\", features = [\"bundled\"] }

# Crypto & Security
ring = "0.17"
sha2 = "0.10"
zeroize = "1.6"
argon2 = "0.5"
keyring = "2.1"
rand = "0.8"
hex = "0.4"

# Document handling
docx-rs = "0.4"
printpdf = { version = "0.7", features = ["embedded_images"] }

# Removed llama-cpp-2 - migrated to pure Rust Candle for reliable Windows builds
# Note: GPU acceleration works without CUDA feature - Candle auto-detects at runtime
# Windows-specific dependencies for GPU detection
[target.'cfg(target_os = "windows")'.dependencies]
wmi = "0.13"
windows = { version = "0.58", features = [
    "Win32_System_Com",
    "Win32_System_Ole",
    "Win32_System_Variant",
    "Win32_Foundation",
] }

[features]
default = ["custom-protocol"]
custom-protocol = ["tauri/custom-protocol"]
# CUDA is optional - only enable if building on system with CUDA toolkit
# For most users, CPU fallback is automatic and works fine
cuda = ["candle-core/cuda"]

[lib]
name = "bear_ai_llm"
path = "src/lib.rs"

[[bin]]
name = "bear-ai-llm"
path = "src/main.rs"

